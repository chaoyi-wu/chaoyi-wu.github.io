<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!--
  <script src="./resources/jsapi" type="text/javascript"></script>
  <script type="text/javascript" async>google.load("jquery", "1.3.2");</script>
 -->

<style type="text/css">
  @font-face {
   font-family: 'Avenir Book';
   src: url("./fonts/Avenir_Book.ttf"); /* File to be stored at your site */
   }

  body {
    font-family: "Avenir Book", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:14px;
    margin-left: auto;
    margin-right: auto;
    width: 800px;
  }
  h1 {
    font-weight:300;
  }
  h2 {
    font-weight:300;
  }

  p {
    font-weight:300;
    line-height: 1.4;
  }

  code {
    font-size: 0.8rem;
    margin: 0 0.2rem;
    padding: 0.5rem 0.8rem;
    white-space: nowrap;
    background: #efefef;
    border: 1px solid #d3d3d3;
    color: #000000;
    border-radius: 3px;
  }

  pre > code {
    display: block;
    white-space: pre;
    line-height: 1.5;
    padding: 0;
    margin: 0;
  }

  pre.prettyprint > code {
    border: none;
  }


  .container {
        display: flex;
        align-items: center;
        justify-content: center
  }
  .image {
        flex-basis: 40%
  }
  .text {
        padding-left: 20px;
        padding-right: 20px;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 0px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;

  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>

	<title>Towards Generalist Foundation Model for Radiology</title>
</head>

<body>
	<br>
	<center>
	<span style="font-size:36px">Towards Generalist Foundation Model for Radiology</span><br><br><br>
	</center>
	<table align="center" width="900px">
            <tbody><tr>
                    <td align="center" width="180px">
              <center>
                <span style="font-size:16px"><a href="https://chaoyi-wu.github.io/">Chaoyi Wu<sup>*</sup></a><sup>1,2</sup></span>
                </center>
                </td>
                    <td align="center" width="180px">
              <center>
                <span style="font-size:16px"><a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang<sup>*</sup></a><sup>1,2</sup></span>
                </center>
                </td>
                    <td align="center" width="180px">
              <center>
                <span style="font-size:16px"><a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a><sup>1,2</sup></span>
                </center>
              </td>
              <td align="center" width="180px">
            
              
	      <center>
                <span style="font-size:16px"><a href="https://mediabrain.sjtu.edu.cn/">Yanfeng Wang</a><sup>1,2</sup></span>
                </center>
              </td>
              <td align="center" width="180px">
        <center>
              <span style="font-size:16px"><a href="https://weidixie.github.io/">Weidi Xie</a><sup>1,2</sup></span>
            </td>
            <td align="center" width="180px">
            </center>
		        
          </tr>
        </tbody></table><br>
	
	  <table align="center" width="700px">
            <tbody><tr>
                    <td align="center" width="50px">
              <center>
                    <span style="font-size:16px"></span>
                </center>
                </td>
                    <td align="center" width="300px">
              <center>
                    <span style="font-size:16px"><sup>1</sup>CMIC, Shanghai Jiao Tong University</span>
                </center>
                </td>
                    <td align="center" width="300px">
              <center>
                    <span style="font-size:16px"><sup>2</sup>Shanghai AI Laboratory</span>
                </center>
                </td>
        </tr></tbody></table>
	
	<table align="center" width="700px">
            <tbody><tr>
              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">Code
                    <a href="https://github.com/chaoyi-wu/RadFM"> [GitHub]</a>
                  </span>
                </center>
              </td>

              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    Paper <a href="https://arxiv.org/abs/2308.02463"> [arXiv]</a>
                  </span>
                </center>
              </td>

              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    Cite <a href="./cite.txt"> [BibTeX]</a>
                  </span>
                </center>
              </td>
            </tr></tbody>
      </table>
	
      <br><hr>
      <!-- <center><h2> Abstract </h2> </center>
      <p style="text-align:justify; text-justify:inter-ideograph;">
      <left>
        In this paper, we consider the problem of enhancing self-supervised visual-language pre-training~(VLP) with medical-specific knowledge, 
        by exploiting the paired image-text reports from the radiological daily practice.
        In particular, we make the following contributions:
        <i>First</i>, unlike existing works that directly process the raw reports,
        we adopt a novel report pre-processing mechanism by simply extracting the useful medical entities, avoiding unnecessary complexity from understanding the language grammar;
        <i>Second</i>, we propose a novel entity embedding module by querying an external knowledge description base, to exploit the rich context of additional information that the medical domain affords, and implicitly build relationships between entities in the language embedding space;
        <i> Third</i>, we propose a novel Transformer-based fusion model for spatially aligning the entity description with visual signals at the image patch level only with self-supervised learning, thus enabling the ability for spatial grounding;
        <i> Fourth</i>, we conduct thorough experiments to validate the effectiveness of our proposed architecture, and benchmark on numerous public benchmarks {\em e.g.}, ChestX-ray14, 
        RSNA Pneumonia, SIIM-ACR Pneumothorax, COVIDx CXR-2, COVID Rural, and EdemaSeverity. 
        In both zero-shot and fine-tuning settings, our model has demonstrated strong performance compared with the former methods on disease classification and grounding.
      </center></p>
      <p><img class="center"  src="./resources/Method.png" width="800px"></p> -->

      <center><h2> Abstract </h2> </center>
        <div class="text" width="800px"> 
          <p style="text-align:justify; text-justify:inter-ideograph;">
            <left>
              In this study, we aim to initiate the development of Radiology Foundation Model, termed as RadFM.
              We consider the construction of foundational models from the perspectives of data, model design, and evaluation thoroughly. 
              Our contribution can be concluded as follows: 
              (<i>i</i>), we construct a large-scale Medical Multi-modal Dataset, MedMD, consisting of 16M 2D and 3D medical scans. 
              To the best of our knowledge, this is the first multi-modal dataset containing 3D medical scans. 
              (<i>ii</i>), We propose an architecture that enables visually conditioned generative pre-training, 
              allowing for the integration of text input interleaved with 2D or 3D medical scans to generate response for diverse radiologic tasks. 
              (<i>iii</i>), we propose a new evaluation benchmark that comprises five tasks, aiming to comprehensively assess the capability of foundation models in handling practical clinical problems. Our experimental results confirm that RadFM significantly outperforms existing multi-modal foundation models. The codes, data, and model checkpoint will all be made publicly available to promote further research and development in the field.
          </p>
        </div>
        <div class="image" width="800px">
          <img style="width:800px" src='./resources/GIF.gif'></img>
        </div>
      <br><hr>
      <center> <h2> Result Overview </h2> </center>
      <p style="text-align:justify; text-justify:inter-ideograph;"><left>
        The general comparison between RadFM and different SOTA methods, i.e., OpenFlamingo and MedVInT
        On the left we plot the radar figure of the three methods, the average of different metrics are plotted and the coordinate axes are
        <b>logarithmized</b>. On the right, we draw the comparison on the five different tasks with different metrics. Both two can indicate
        the superiority of RadFM, surpassing former methods significantly.
	      </left></p>
        <p><img class="left"  src="./resources/Quick_Results.jpg" width="800px"></p>
      
      <br>
      <hr>
      <center><h2>Datasets</h2></center>
      <p><b><h3>Bird-view of MedMD</h3></b> </p>
      <p><left>
        Overview of Medical Multimodal Dataset (MedMD). Our collected data covers the majority of radiologic modalities
        and anatomical regions of the human body, such as brain, head and neck, thorax, spine, abdomen, upper limb, lower limb, and
        pelvis, etc. The dataset mixes two types of datasets, i.e., interleaved datasets and visual instruction datasets, including both 3D and 2D scans. T refers to the
        text of interleaved data, I refers to the instruction input text, and R refers to the response text. 
      </left></p>
      <p><img class="center"  src="./resources/whole_body.jpg" width="800px"></p>
      <p> The individual componet  of MedMD. 
        Differring from previous multimodal datasets, our dataset is visual-language interleaved and contains 3D data. 
        To the best of our knowledge, this is probably the largest open-source medical multi-modal dataset available. 
        In table, more detailed case numbers are shown. 
      </p>
      <p><img class="center"  src="./resources/Table_MedMD.jpg" width="800px"></p>

      <p><b><h3>The radiologic filtered version, RadMD</h3></b> </p>
      <p><left>
      For domain-specific finetuning, we filter out the non-radiology images from MedMD, and construct a clean
      subset, named Radiology Multimodal Dataset (RadMD), dedicating to supervised visual instruction-tuning.
      It contains a total of 3M images, spanning various data formats, modalitieis, and tasks, as shown in the figure. More comprehensive details regarding the filtering process and the
      resulting dataset sizes can be found in following table.
      </left></p>
      
      <p><img class="left"  src="./resources/portion_fig.jpg" width="800px"></p>
      <p><img class="center"  src="./resources/Table_RadMD.jpg" width="800px"></p>



      <br><hr>
      <center><h2>Towards Building Generalist Foundation Model for Radiology</h2></center>
      <p><left>
      we propose a learning paradigm for unifying different medical tasks into a
      generative framework, and, then, we first pre-train the model on MedMD, following fine-tuned on RadMD for domain-specific adaptation. 
      The c part in figure shows our main architecture, we unified encoding 2D and 3D images, and multi-modal encoding by insert image embedding into texts.
      </left></p>
      <p><img class="center"  src="./resources/Workflow.jpg" width="800px"></p>

      <br><hr>

      <center><h2>Final Results</h2></center>
      <p><b><h3>Quantitative Results</h3></b> </p>
      <p><left>
        RadFM outperforms previous methods by a significant margin across all five
        tasks, showcasing its exceptional capabilities. Notably, RadFM excels in particularly challenging tasks such as
        medical VQA, report generation, and rationale diagnosis, which demand profound understanding of both
        textual information and images. 
      </left></p>
      <p><img class="center"  src="./resources/Final_result.jpg" width="800px"></p>

      <p><b><h3>Case Study</h3></b> </p>

      <p><left>
        In medical VQA, RadFM demonstrates its ability to comprehend the questions and provide
        answers in a consistent format, accurately addressing the questions. However, in some challenging cases,
        such as the first example where the question pertains to the type of abnormality, the model faces difficulty
      predicting "ectopic ACTH-producing tumor" and mistakenly identifies it as “primary lung neoplasm”, that
      requires fine-grained discrimination within tumor types.
      </left></p>
      <p><img class="center"  src="./resources/case_1.jpg" width="800px"></p>

      <p><left>
        Additionally, for report generation, we also show several cases. The
        blue color denotes instances where the model missed this information that has been mentioned in the reference reports. 
        In general, RadFM can be observed
        that the model is capable of identifying fundamental diseases and, in some cases, performs exceptionally well.
        However, the report generated by MedFM may lack specific location information like the 'left' or 'right' of an
        anatomy region.

      </left></p>
      <p><img class="center"  src="./resources/case_2.jpg" width="800px"></p>

      <p><left>
        Finaly for rationale diagnosis, on both cases, RadFM can make accurate diagnosis in free form and give
        further related radiologic reasoning. However, the limitation can also be observed that the reasoning results
        are still general and more like background medical knowledge, yet not specific to the input case.
      </left></p>
      <p><img class="center"  src="./resources/case_3.jpg" width="800px"></p>

      <br><hr>
      <center><h2>Conclusion</h2></center>
      <p><left>
        In conclusion, in this paper, we have constructed a complete set of medical foundation model-building processes,
        including data collection, problem formulation, model design, training, and evaluation. We construct the
        largest medical multi-modal database in this paper and in model capabilities, compared to existing work,
        our model is able to process multiple 3D or 2D image inputs interleaved with texts, which fits the practical
        usage more. We surpass the latest open-source multi-modal foundation model significantly. We will release
        all corresponding data, codes, and models. We believe this can greatly promote the development of medical
        foundation models.
      </left></p>
</body>
</html>
