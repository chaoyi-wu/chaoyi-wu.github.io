<!DYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Chaoyi Wu</title>
  
  <meta name="author" content="Chaoyi Wu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name> Chaoyi Wu</name>
              </p>
              <p>I am a PhD student working on medical image analysis and machine learning at <a href="https://mediabrain.sjtu.edu.cn/">Shanghai Jiao Tong University</a>.
              </p>
	      <p>My current research interest is in multimodal learning for medical image analysis.</p> 
              <a href="wtzxxxwcy02@sjtu.edu.cn">Email</a> &nbsp/&nbsp
              <a href="https://scholar.google.com/citations?user=ZLHTzHEAAAAJ&hl=zh-CN">Scholar</a> &nbsp/&nbsp
              <a href="https://github.com/chaoyi-wu/">Github</a>
              <p style="text-align:center">
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="image/Chaoyi.JPG"><img style="width:100%;max-width:100%" alt="profile photo" src="image/Chaoyi.JPG" class="hoverZoomLink"></a>
            </td>
          </tr>
         
	

  
</tbody></table>
	<br />

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading><b></bold>Research</b></heading>

	  <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/SAT.png" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2312.17183">
                <papertitle>One Model to Rule them All: Towards Universal Segmentation for Medical Images with Text Prompts</papertitle>
              </a>
              <br>
              <a>Ziheng Zhao</a>,
              <a href="https://github.com/YaoZhang93">Yao Zhang</a>,,
              <strong>Chaoyi Wu</strong>,
              <a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang<sup>*</sup></a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang</a>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>,
              <br>
              <em>Technical Report, 2023.</em>
              <p>
               In this paper, we build up a universal medical segmentation model, driven by text prompts (SAT).
              </p>
            </td>
          </tr> 
		
          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/RP3D-Diag.png" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2312.16151">
                <papertitle>Large-scale Long-tailed Disease Diagnosis on Radiology Images</papertitle>
              </a>
              <br>
              <a>Qiaoyu Zheng<sup>*</sup></a>,
              <a>Weike Zhao<sup>*</sup></a>,
              <strong>Chaoyi Wu<sup>*</sup></strong>,
              <a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang<sup>*</sup></a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang</a>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>,
              <br>
              <em>Technical Report, 2023.</em>
              <p>
               In this paper, we collect a large-scale multi-modal, multi-scan, long-tailed muti-lable diagnosis (classification) dataset. We further propose a vision-encoder together with a fusion module, enabling arbitary scan input per case. On evaluation, our methods achive better experiment results on our benchmark and can also serve as an pre-train mdoel for external datasets.
              </p>
            </td>
          </tr> 

          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/GPT_4V_Eval.png" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2310.09909">
                <papertitle>Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis</papertitle>
              </a>
              <br>
              <strong>Chaoyi Wu<sup>*</sup></strong>,
              <a>Jiayu Lei<sup>*</sup></a>,
              <a>Qiaoyu Zheng<sup>*</sup></a>,
              <a>Weike Zhao<sup>*</sup></a>,
              <a>Weixiong Lin<sup>*</sup></a>,
              <a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang<sup>*</sup></a>,
              <a>Xiao Zhou<sup>*</sup></a>,
              <a>Ziheng Zhao<sup>*</sup></a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang</a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>,
              <br>
              <em>Technical Report, 2023.</em>
              <p>
               We evaluate the GPT-4V on 92 radiographic cases, 20 pathoglogy cases and 16 location cases across 17 medical systems covering 8 imaging modalities. In general, as the cases shown, GPT-4V is still  far from clinical usage.
              </p>
            </td>
          </tr> 
          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/UniBrain.png" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2309.06828.pdf">
                <papertitle>UniBrain: Universal Brain MRI Diagnosis with Hierarchical Knowledge-enhanced Pre-training</papertitle>
              </a>
              <br>
              <a>Jiayu Lei</a>,
              <a>Lisong Dai</a>,
              <a>Haoyun Jiang</a>,
              <strong>Chaoyi Wu</strong>,
              <a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang</a>,
              <a href="https://github.com/YaoZhang93">Yao Zhang</a>,
              <a href="https://sunarker.github.io/index.html">Jiangchao Yao</a>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>,
              <a>Yanyong Zhang</a>,
              <a>Yuehua Li</a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a>,
              <br>
              <em>Technical Report, 2023.</em>
              <p>
               We release a new knowledge-enhanced Brain MRI pre-train foundation model leveraging image-report pairs which can realize zero-shot diagnosis of unseen brain diseases. 
              </p>
            </td>
          </tr> 
          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/RadFM.jpg" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2308.02463">
                <papertitle>Towards Generalist Foundation Model for Radiology by Leveraging Web-scale 2D&3D Medical Data</papertitle>
              </a>
              <br>
              <strong>Chaoyi Wu<sup>*</sup></strong>,
              <a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang<sup>*</sup></a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>,
              <br>
              <em>Technical Report, 2023.</em>
              <p>
                In this study, we aim to initiate the development of Radiology Foundation Model, termed as RadFM.
                we construct a large-scale Medical Multi-modal Dataset, MedMD, consisting of 16M 2D and 3D medical scans. 
              </p>
            </td>
          </tr> 

          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/PMC-VQA.png" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2305.10415.pdf">
                <papertitle>PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering</papertitle>
              </a>
              <br>
              <a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang<sup>*</sup></a>,
              <strong>Chaoyi Wu<sup>*</sup></strong>,
              <a>Weixiong Lin</a>,
              <a>Ziheng Zhao</a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>,
              <br>
              <em>Technical Report, 2023.</em>
              <p>
                In this paper, we focus on the problem of Medical Visual Question Answering (MedVQA). We propose a generative medical VQA model, MedVInT, together with a large scale MedVQA Dataset, PMC-VQA.
              </p>
            </td>
          </tr>

          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/PMC-LLaMA.png" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2304.14454">
                <papertitle>PMC-LLaMA: Towards Building Open-source Language Models for Medicine</papertitle>
              </a>
              <br>
              <strong>Chaoyi Wu</strong>,
              <a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang</a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>,
              <br>
              <em>Technical Report, 2023.</em>
              <p>
              In this report, we introduce PMC-LLaMA, an open-source language model that is acquired leveraging large medical corpus, surpassing chatGPT on medicalQA benchmarks.
              </p>
            </td>
          </tr>

          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/PMC_CLIP.png" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2303.07240">
                <papertitle>PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents</papertitle>
              </a>
              <br>
              <a>Weixiong Lin</a>,
              <a>Ziheng Zhao</a>,
              <a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang</a>,
              <strong>Chaoyi Wu</strong>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>,
              <br>
              <em>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2023.</em>
              <p>
              We collect a biomedical dataset, PMC-OA with <strong>1.6M</strong> image-caption pairs collected from PubMedCentral's OpenAccess subset.
              </p>
            </td>
          </tr>

          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/KAD.png" width="250" height="150" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2302.14042">
                <papertitle>Knowledge-enhanced Pre-training for Auto-diagnosis of Chest Radiology Images</papertitle>
              </a>
              <br>
              <a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang</a>,
              <strong>Chaoyi Wu</strong>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>,
              <br>
              <em>Nature Communications, 2023.</em>
              <p>
              Here, we propose a knowledge-enhanced vision-language pre-training approach for auto-diagnosis on chest X-ray images. First trains a knowledge encoder based on an existing medical knowledge graph, 
              then leverages the pre-trained knowledge encoder to guide the visual representation learning. 
              </p>
            </td>
          </tr>
            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/K-Diag.png" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2302.11557">
                <papertitle>K-Diag: Knowledge-enhanced Disease Diagnosis in Radiographic Imaging</papertitle>
              </a>
              <br>
              <strong>Chaoyi Wu<sup>*</sup></strong>,
              <a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang<sup>*</sup></a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>,
              <br>
              <em>MICCAI-BTSD (workshop), 2023, Oral.</em>
              <p>
              In this paper, we consider the problem of disease diagnosis. Unlike the conventional learning paradigm that treats labels independently, we propose a knowledge-enhanced framework, that enables training visual representation with the guidance of medical domain knowledge.
              </p>
            </td>
          </tr> 
            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/MedKLIP.png" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2301.02228">
                <papertitle>MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training</papertitle>
              </a>
              <br>
              <strong>Chaoyi Wu</strong>,
              <a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang</a>,
	            <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>,
             
              <br>
              <em>International Conference on Computer Vision (ICCV), 2023.</em>
              <p>
              We propose to leverage medical specific knowledge enhancing language-image pre-training method, significantly advancing the ability of pre-trained models to handle unseen diseases on zero-shot classification and grounding tasks.
              </p>
            </td>
         
            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
              <td style="padding:0px;width:25%;vertical-align:middle">
                <img src="image/BE-SLL.png" width="250" style="border-style: none">
              </td>
              <td style="padding:10px;width:75%;vertical-align:middle">
                <a href="https://link.springer.com/chapter/10.1007/978-3-031-16431-6_2">
                  <papertitle>Boundary-Enhanced Self-supervised Learning for Brain Structure Segmentation</papertitle>
                </a>
                <br>
                <a >Feng Chang</a>,
                <strong>Chaoyi Wu</strong>,
                <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a>,
                
                <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
                <a >Xin Chen</a>,
                <a >Qi Tian</a>,
                <br>
                <em>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2022.</em> 
                <p>
                We propose Boundary-Enhanced Self-SupervisedLearning (BE-SSL), leveraging supervoxel segmentation and registrationas two related proxy tasks, enhancing brain structure segmentation.
                </p>
              </td>
            </tr>
          </tr> 
          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/MLNdetection.png" width="250" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://www.sciencedirect.com/science/article/pii/S0895611122000805">
                <papertitle>Integrating features from lymph node stations for metastatic lymph node detection</papertitle>
              </a>
              <br>
              <strong>Chaoyi Wu</strong>,
              <a >Feng Chang</a>,
              <a >Xiao Su</a>,
              <a >Zhihan Wu</a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a>,
              <a >Ling Zhu</a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              
              <br>
              <em>Computerized Medical Imaging and Graphics (CMIG), 2022, 101: 102108.</em>
              <p>
              We first leverage the information of LN stations for metastatic LN detection. Metastatic LN station classification is proposed as proxy task for metastatic LN detection. A GCN-based structure is adopted to model the mutual influence among LN stations.
              </p>
            </td>
          </tr>  
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
		Based on a template by <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
