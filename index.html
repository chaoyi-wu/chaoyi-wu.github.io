<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Chaoyi Wu</title>
  
  <meta name="author" content="Chaoyi Wu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <style>
        name {
            font-size: 32px;  /* 放大字体 */
            font-weight: bold; /* 加粗 */
            display: block;    /* 确保样式生效 */
        }
        .circle-image {
            width: 300px;
            height: 300px;
            border-radius: 50%;
            object-fit: cover;
            overflow: hidden;
            border: 2px solid #ccc;
        }

        a {
            display: inline-block;
        }

        .publication-item {
            margin-bottom: 20px;
            padding: 15px;
            border-left: 3px solid #ddd;
            background-color: #f9f9f9;
        }

        .publication-title {
            font-weight: bold;
            font-size: 16px;
            margin-bottom: 8px;
            color: #333;
        }

        .publication-authors {
            margin-bottom: 5px;
            color: #666;
        }

        .publication-venue {
            font-style: italic;
            margin-bottom: 8px;
            color: #007acc;
        }

        .publication-description {
            color: #555;
            line-height: 1.4;
        }

        .year-header {
            text-align: left;
            font-weight: bold;
            font-size: 20px;
            font-style: italic;
            padding-bottom: 20px;
            padding-top: 30px;
            border-bottom: 2px solid #ddd;
            margin-bottom: 20px;
        }
    </style>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name> Chaoyi Wu</name>
              </p>
              <p>
				  Hi! I'm Chaoyi, an Assistant Professor at the 
				  <a href="https://soai.sjtu.edu.cn/cn/facultydetails/zzjs/wuchaoyi">
				    School of Artificial Intelligence, Shanghai Jiao Tong University
				  </a>, specializing in AI for Medicine.
				</p>
				<p>
				 My current research focuses on advancing medical foundation models in both language and multimodal domains, and on designing agentic systems that push the boundaries of AI4Medicine. 
				</p>
				<p>
				  I'm looking for enthusiastic Master's and PhD students to join our team! If you're passionate about medical AI and motivated to explore new frontiers, I'd love to hear from you and work together to shape the future.
				</p>
              <a href="wtzxxxwcy02@sjtu.edu.cn">Email</a> &nbsp/&nbsp
              <a href="https://scholar.google.com/citations?user=ZLHTzHEAAAAJ&hl=zh-CN">Scholar</a> &nbsp/&nbsp
              <a href="https://github.com/chaoyi-wu/">Github</a>
              <p style="text-align:center">
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="image/Chaoyi.jpg"><img class="circle-image" alt="profile photo" src="image/Chaoyi.jpg"></a>
            </td>
          </tr>
        </tbody></table>
	<br />

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td>

              <heading style="position: relative;"><b>Latest Highlight❗</b>
          
          <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                <td style="padding:0px;width:25%;vertical-align:middle">
                    <img src="image/deepdxsearch.jpg" width="250" style="border-style: none">
                </td>
                <td style="padding:10px;width:75%;vertical-align:middle">
                    <a href="#">
                        <papertitle>End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning</papertitle>
                    </a>
                    <br>
                    <a>Qiaoyu Zheng</a>,
                    <a>Yuze Sun</a>,
                    <strong>Chaoyi Wu</strong>,
                    <a>Weike Zhao</a>,
                    <a>Pengcheng Qiu</a>,
                    <a>Yongguo Yu</a>,
                    <a>Kun Sun</a>,
                    <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang</a>,
                    <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
                    <a href="https://weidixie.github.io/">Weidi Xie</a>
                    <br>
                    <em>Technical Report, 2025</em>
                    <p>
                        In this paper, we propose an end-to-end reinforcement learning framework for training agentic RAG systems, evolving their action policies through large-scale data fitting to achieve enhanced traceable diagnostic reasoning capabilities.
                    </p>
                </td>
            </tr>

        	<tr onmouseout="ld_stop()" onmouseover="ld_start()">
                      <td style="padding:0px;width:25%;vertical-align:middle">
                        <img src="image/DeepRare.jpg" width="250" style="border-style: none">
                      </td>
                      <td style="padding:10px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2506.20430">
                          <papertitle>An Agentic System for Rare Disease Diagnosis with Traceable Reasoning</papertitle>
                        </a>
                        <br>
                        <a>Weike Zhao*</a>,
                        <strong>Chaoyi Wu*</strong>,
                        <a>Yanjie Fan*</a>,
                        <a href="https://xiaoman-zhang.github.io/"> Xiaoman Zhang</a>,
                        <a>Pengcheng Qiu</a>,
                        <a>Yuze Sun</a>,
                        <a>Xiao Zhou</a>,
                        <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang</a>,
                        <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
                        <a>Yongguo Yu</a>,  
                        <a>Kun Sun</a>,  
                       
                        <a href="https://weidixie.github.io/">Weidi Xie</a>,
                        <br>
                        <em> Technical Report, 2025.</em>
                        <p>
                          We develop DeepRare, the first agentic AI system for rare disease diagnosis that integrates specialized tools and medical knowledge sources to provide traceable diagnostic reasoning with exceptional accuracy across multiple evaluation datasets.
                        </p>
                      </td>
                  </tr>

            <tr onmouseout="ld_stop()" onmouseover="ld_start()">
                  <td style="padding:0px;width:25%;vertical-align:middle">
                      <img src="image/MedRBench.jpg" width="250" style="border-style: none">
                  </td>
                  <td style="padding:10px;width:75%;vertical-align:middle">
                      <a href="https://arxiv.org/pdf/2503.04691">
                          <papertitle>Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases</papertitle>
                      </a>
                      <br>
                      <a>Pengcheng Qiu*</a>,
                      <strong>Chaoyi Wu*</strong>,
                      <a>Pengcheng Qiu</a>,
                      <a>Shuyu Liu</a>,
                      <a>Weike Zhao</a>,
                      <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
                      <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang</a>,
                      <a href="https://weidixie.github.io/">Weidi Xie</a>
                      <br>
                      <em>Technical Report, 2025</em>
                      <p>
                          In this study, we quantitatively evaluate the free-text reasoning abilities of various state-of-the-art LLMs, such as DeepSeek-R1 and OpenAI-o3-mini, in assessment recommendation, diagnostic decision, and treatment planning.
                      </p>
                  </td>
              </tr>
        </tbody></table>

	<br />

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td>
              <heading><b>Research</b></heading> 
              <br/><p></p>

              <div class="year-header">2025</div>
              
              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/pdf/2503.04691">Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases</a>
                  </div>
                  <div class="publication-authors">
                      Pengcheng Qiu*, <strong>Chaoyi Wu*</strong>, Pengcheng Qiu, Shuyu Liu, Weike Zhao, Ya Zhang, Yanfeng Wang, Weidi Xie
                  </div>
                  <div class="publication-venue">Technical Report, 2025</div>
                  <div class="publication-description">
                      In this study, we quantitatively evaluate the free-text reasoning abilities of various state-of-the-art LLMs, such as DeepSeek-R1 and OpenAI-o3-mini, in assessment recommendation, diagnostic decision, and treatment planning.
                  </div>
              </div>

              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/abs/2502.20301">M^3Builder: A Multi-Agent System for Automated Machine Learning in Medical Imaging</a>
                  </div>
                  <div class="publication-authors">
                      Jinghao Feng*, Qiaoyu Zheng*, <strong>Chaoyi Wu</strong>, Ziheng Zhao, Ya Zhang, Yanfeng Wang, Weidi Xie
                  </div>
                  <div class="publication-venue">MICCAI2025-Workshop, Oral</div>
                  <div class="publication-description">
                      In this study, we develop an automated machine learning agentic AI system for medical imaging analysis, aiming to equip medical agent systems with self-evolution capabilities.
                  </div>
              </div>

              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/abs/2503.04653">RadIR: A Scalable Framework for Multi-Grained Medical Image Retrieval via Radiology Report Mining</a>
                  </div>
                  <div class="publication-authors">
                      Tengfei Zhang*, Ziheng Zhao*, <strong>Chaoyi Wu</strong>, Xiao Zhou, Ya Zhang, Yanfeng Wang, Weidi Xie
                  </div>
                  <div class="publication-venue">MICCAI2025, Early Accepted</div>
                  <div class="publication-description">
                      In this study, we propose a novel medical image similarity ordering pipeline that operates at multiple granularities by effectively utilizing rich information extracted from dense radiology report annotations.
                  </div>
              </div>

              <div class="year-header">2024</div>

              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/pdf/2412.09529">Can Modern LLMs Act as Agent Cores in Radiology Environments?</a>
                  </div>
                  <div class="publication-authors">
                      Qiaoyu Zheng*, <strong>Chaoyi Wu*</strong>, Pengcheng Qiu, Lisong Dai, Ya Zhang, Yanfeng Wang, Weidi Xie
                  </div>
                  <div class="publication-venue">Technical Report, 2024</div>
                  <div class="publication-description">
                      In this study, we systematically investigate a pre-requisite question for building concrete radiology agents which is, 'Can modern LLMs act as agent cores in radiology environments?' Serving for this goal, we build up RadABench, a comprehensive LLM-based agent evaluation benchmark for radiology.
                  </div>
              </div>

              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/abs/2408.12547">Towards Evaluating and Building Versatile Large Language Models for Medicine</a>
                  </div>
                  <div class="publication-authors">
                      <strong>Chaoyi Wu*</strong>, Pengcheng Qiu*, Jinxin Liu, Hongfei Gu, Na Li, Ya Zhang, Yanfeng Wang, Weidi Xie
                  </div>
                  <div class="publication-venue">npj Digital Medicine, 2025</div>
                  <div class="publication-description">
                      In this study, we present MedS-Bench, a comprehensive benchmark designed to evaluate the performance of large language models (LLMs) in clinical contexts beyond multiple-choice question-answering. Moreover, we build up a new comprehensive medical instruction dataset, termed as MedS-Ins.
                  </div>
              </div>

              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/abs/2406.16845">AutoRG-Brain: Grounded Report Generation for Brain MRI</a>
                  </div>
                  <div class="publication-authors">
                      Jiayu Lei, Xiaoman Zhang, <strong>Chaoyi Wu</strong>, Ya Zhang, Yanfeng Wang, Weidi Xie
                  </div>
                  <div class="publication-venue">Computerized Medical Imaging and Graphics (CMIG), 2024</div>
                  <div class="publication-description">
                      In this paper, we propose a grounded report generation system for brain MRI leveraging the cooperation of different sub-tools. In real clinical scenarios, the system can significantly improve the efficiency of the radiologists.
                  </div>
              </div>

              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/abs/2406.16845">RaTEScore: A Metric for Radiology Report Generation</a>
                  </div>
                  <div class="publication-authors">
                      Weike Zhao, <strong>Chaoyi Wu</strong>, Ya Zhang, Yanfeng Wang, Weidi Xie
                  </div>
                  <div class="publication-venue">EMNLP2024, Main</div>
                  <div class="publication-description">
                      In this paper, we propose an entity-level assessment metric for radiological reports beyond chest x-ray using NER and synonym normalization models. Unlike LLM-based assessment pipelines, our metric is more lightweight and objective targeting large-scale auto-evaluation.
                  </div>
              </div>

              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/abs/2402.13963">RadGenome-Chest CT: A Grounded Vision-Language Dataset for Chest CT Analysis</a>
                  </div>
                  <div class="publication-authors">
                      Xiaoman Zhang, <strong>Chaoyi Wu</strong>, Ziheng Zhao, Jiayu Lei, Ya Zhang, Yanfeng Wang, Weidi Xie
                  </div>
                  <div class="publication-venue">Technical Report, 2024</div>
                  <div class="publication-description">
                      In this paper, we introduce RadGenome-Chest CT, a comprehensive, large-scale, region-guided 3D chest CT interpretation dataset based on CT-RATE. It includes: Organ-level segmentation for 197 categories; 665K multi-granularity grounded reports; 1.3M grounded VQA pairs.
                  </div>
              </div>

              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/abs/2402.13963">Knowledge-enhanced Visual-Language Pretraining for Computational Pathology</a>
                  </div>
                  <div class="publication-authors">
                      Xiao Zhou, Xiaoman Zhang, <strong>Chaoyi Wu</strong>, Ya Zhang, Yanfeng Wang, Weidi Xie
                  </div>
                  <div class="publication-venue">ECCV2024 Oral</div>
                  <div class="publication-description">
                      In this paper, we consider the problem of visual representation learning for computational pathology, by exploiting large-scale image-text pairs gathered from public resources, along with the domain specific knowledge in pathology.
                  </div>
              </div>

              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/abs/2402.13963">Towards Building Multilingual Language Model for Medicine</a>
                  </div>
                  <div class="publication-authors">
                      Pengcheng Qiu*, <strong>Chaoyi Wu*</strong>, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, Weidi Xie
                  </div>
                  <div class="publication-venue">Nature Communications, 2024</div>
                  <div class="publication-description">
                      In this paper, we aim to develop a multilingual language corpus (MMedC), benchmark (MMedBench) and an open-source, multilingual language model (MMedLM) for medicine, that benefits a wider, linguistically diverse audience from different regions.
                  </div>
              </div>

              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/abs/2312.17183">One Model to Rule them All: Towards Universal Segmentation for Medical Images with Text Prompts</a>
                  </div>
                  <div class="publication-authors">
                      Ziheng Zhao, Yao Zhang, <strong>Chaoyi Wu</strong>, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie
                  </div>
                  <div class="publication-venue">npj Digital Medicine, 2025</div>
                  <div class="publication-description">
                      In this paper, we build up a universal medical segmentation model, driven by text prompts (SAT).
                  </div>
              </div>

              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/abs/2312.16151">Large-scale Long-tailed Disease Diagnosis on Radiology Images</a>
                  </div>
                  <div class="publication-authors">
                      Qiaoyu Zheng*, Weike Zhao*, <strong>Chaoyi Wu*</strong>, Xiaoman Zhang*, Ya Zhang, Yanfeng Wang, Weidi Xie
                  </div>
                  <div class="publication-venue">Nature Communications, 2024</div>
                  <div class="publication-description">
                      In this paper, we collect a large-scale multi-modal, multi-scan, long-tailed multi-label diagnosis (classification) dataset. We further propose a vision encoder together with a fusion module, enabling arbitrary scan input per case. On evaluation, our methods achieve better experiment results on our benchmark and can also serve as a pre-train model for external datasets.
                  </div>
              </div>

              <div class="year-header">2023</div>

              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/abs/2310.09909">Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis</a>
                  </div>
                  <div class="publication-authors">
                      <strong>Chaoyi Wu*</strong>, Jiayu Lei*, Qiaoyu Zheng*, Weike Zhao*, Weixiong Lin*, Xiaoman Zhang*, Xiao Zhou*, Ziheng Zhao*, Yanfeng Wang, Ya Zhang, Weidi Xie
                  </div>
                  <div class="publication-venue">Technical Report, 2023</div>
                  <div class="publication-description">
                      We evaluate the GPT-4V on 92 radiographic cases, 20 pathology cases and 16 location cases across 17 medical systems covering 8 imaging modalities. In general, as the cases shown, GPT-4V is still far from clinical usage.
                  </div>
              </div>

              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/pdf/2309.06828.pdf">UniBrain: Universal Brain MRI Diagnosis with Hierarchical Knowledge-enhanced Pre-training</a>
                  </div>
                  <div class="publication-authors">
                      Jiayu Lei, Lisong Dai, Haoyun Jiang, <strong>Chaoyi Wu</strong>, Xiaoman Zhang, Yao Zhang, Jiangchao Yao, Weidi Xie, Yanyong Zhang, Yuehua Li, Ya Zhang, Yanfeng Wang
                  </div>
                  <div class="publication-venue">Computerized Medical Imaging and Graphics (CMIG), 2025</div>
                  <div class="publication-description">
                      We release a new knowledge-enhanced Brain MRI pre-train foundation model leveraging image-report pairs which can realize zero-shot diagnosis of unseen brain diseases.
                  </div>
              </div>

              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/abs/2308.02463">Towards Generalist Foundation Model for Radiology by Leveraging Web-scale 2D&3D Medical Data</a>
                  </div>
                  <div class="publication-authors">
                      <strong>Chaoyi Wu*</strong>, Xiaoman Zhang*, Yanfeng Wang, Ya Zhang, Weidi Xie
                  </div>
                  <div class="publication-venue">Nature Communications</div>
                  <div class="publication-description">
                      In this study, we aim to initiate the development of Radiology Foundation Model, termed as RadFM. We construct a large-scale Medical Multi-modal Dataset, MedMD, consisting of 16M 2D and 3D medical scans.
                  </div>
              </div>

              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/pdf/2305.10415.pdf">Development of a large-scale medical visual question-answering dataset</a>
                  </div>
                  <div class="publication-authors">
                      Xiaoman Zhang*, <strong>Chaoyi Wu*</strong>, Weixiong Lin, Ziheng Zhao, Yanfeng Wang, Ya Zhang, Weidi Xie
                  </div>
                  <div class="publication-venue">Nature Communications Medicine, 2024</div>
                  <div class="publication-description">
                      In this paper, we focus on the problem of Medical Visual Question Answering (MedVQA). We propose a generative medical VQA model, MedVInT, together with a large scale MedVQA Dataset, PMC-VQA.
                  </div>
              </div>

              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/abs/2304.14454">PMC-LLaMA: Towards Building Open-source Language Models for Medicine</a>
                  </div>
                  <div class="publication-authors">
                      <strong>Chaoyi Wu</strong>, Xiaoman Zhang, Yanfeng Wang, Ya Zhang, Weidi Xie
                  </div>
                  <div class="publication-venue">Journal of the American Medical Informatics Association (JAMIA)</div>
                  <div class="publication-description">
                      In this report, we introduce PMC-LLaMA, an open-source language model that is acquired leveraging large medical corpus, surpassing chatGPT on medicalQA benchmarks.
                  </div>
              </div>

              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/pdf/2303.07240">PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents</a>
                  </div>
                  <div class="publication-authors">
                      Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, <strong>Chaoyi Wu</strong>, Yanfeng Wang, Ya Zhang, Weidi Xie
                  </div>
                  <div class="publication-venue">International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2023</div>
                  <div class="publication-description">
                      We collect a biomedical dataset, PMC-OA with <strong>1.6M</strong> image-caption pairs collected from PubMedCentral's OpenAccess subset.
                  </div>
              </div>

              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/abs/2302.14042">Knowledge-enhanced Pre-training for Auto-diagnosis of Chest Radiology Images</a>
                  </div>
                  <div class="publication-authors">
                      Xiaoman Zhang, <strong>Chaoyi Wu</strong>, Yanfeng Wang, Ya Zhang, Weidi Xie
                  </div>
                  <div class="publication-venue">Nature Communications, 2023</div>
                  <div class="publication-description">
                      Here, we propose a knowledge-enhanced vision-language pre-training approach for auto-diagnosis on chest X-ray images. First trains a knowledge encoder based on an existing medical knowledge graph, then leverages the pre-trained knowledge encoder to guide the visual representation learning.
                  </div>
              </div>

              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/abs/2302.11557">K-Diag: Knowledge-enhanced Disease Diagnosis in Radiographic Imaging</a>
                  </div>
                  <div class="publication-authors">
                      <strong>Chaoyi Wu*</strong>, Xiaoman Zhang*, Yanfeng Wang, Ya Zhang, Weidi Xie
                  </div>
                  <div class="publication-venue">MICCAI2023-Workshop, Oral</div>
                  <div class="publication-description">
                      In this paper, we consider the problem of disease diagnosis. Unlike the conventional learning paradigm that treats labels independently, we propose a knowledge-enhanced framework, that enables training visual representation with the guidance of medical domain knowledge.
                  </div>
              </div>

              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/abs/2301.02228">MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training</a>
                  </div>
                  <div class="publication-authors">
                      <strong>Chaoyi Wu</strong>, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie
                  </div>
                  <div class="publication-venue">International Conference on Computer Vision (ICCV), 2023</div>
                  <div class="publication-description">
                      We propose to leverage medical specific knowledge enhancing language-image pre-training method, significantly advancing the ability of pre-trained models to handle unseen diseases on zero-shot classification and grounding tasks.
                  </div>
              </div>

              <div class="year-header">2022</div>

              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://link.springer.com/chapter/10.1007/978-3-031-16431-6_2">Boundary-Enhanced Self-supervised Learning for Brain Structure Segmentation</a>
                  </div>
                  <div class="publication-authors">
                      Feng Chang, <strong>Chaoyi Wu</strong>, Yanfeng Wang, Ya Zhang, Xin Chen, Qi Tian
                  </div>
                  <div class="publication-venue">International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2022</div>
                  <div class="publication-description">
                      We propose Boundary-Enhanced Self-Supervised Learning (BE-SSL), leveraging supervoxel segmentation and registration as two related proxy tasks, enhancing brain structure segmentation.
                  </div>
              </div>

              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://www.sciencedirect.com/science/article/pii/S0895611122000805">Integrating features from lymph node stations for metastatic lymph node detection</a>
                  </div>
                  <div class="publication-authors">
                      <strong>Chaoyi Wu</strong>, Feng Chang, Xiao Su, Zhihan Wu, Yanfeng Wang, Ling Zhu, Ya Zhang
                  </div>
                  <div class="publication-venue">Computerized Medical Imaging and Graphics (CMIG), 2022, 101: 102108</div>
                  <div class="publication-description">
                      We first leverage the information of LN stations for metastatic LN detection. Metastatic LN station classification is proposed as proxy task for metastatic LN detection. A GCN-based structure is adopted to model the mutual influence among LN stations.
                  </div>
              </div>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
		Based on a template by <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
